{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis \n",
    "\n",
    "PCA is a dimensionality reduction technique; it lets you distill multi-dimensional data down to fewer dimensions, selecting new dimensions that preserve variance in the data as best it can.\n",
    "\n",
    "We Will See How To Implement it in this python notebook with the diffrent way: Using Our Pure Code , With Numpy and With Sklearn library But First Let's Take a Look How PCA Work Under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Approach \n",
    "\n",
    "1. Standardize the data.\n",
    "- Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector   Decomposition.\n",
    "- Sort eigenvalues in descending order and choose the $k$ eigenvectors that correspond to the $k$ largest eigenvalues where $k$ is the number of dimensions of the new feature subspace $(k≤d)$.\n",
    "- Construct the projection matrix $W$ from the selected $k$ eigenvectors.\n",
    "- Transform the original dataset $X$ via $W$ to obtain a $k$-dimensional feature subspace $Y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Standardizing The Data\n",
    "\n",
    "Most of the times, our dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem.\n",
    "\n",
    "If left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.\n",
    "\n",
    "Whether to standardize the data prior to a PCA on the covariance matrix depends on the measurement scales of the original features. Since PCA yields a feature subspace that maximizes the variance along the axes, it makes sense to standardize the data, especially, if it was measured on different scales.\n",
    "\n",
    "<img src = \"https://cdn-images-1.medium.com/max/1600/1*EyPd0sQxEXtTDSJgu72JNQ.jpeg\" title =\"Tiny Features vs Mega Features\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Computing Eigenvectors and Eigenvalues\n",
    "\n",
    "The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the \"core\" of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.\n",
    "\n",
    "#### Covariance Matrix\n",
    "\n",
    "The classic approach to PCA is to perform the eigendecomposition on the covariance matrix $Σ$, which is a $n×n$ matrix where each element represents the covariance between two features. The covariance between two features can be calculated like the following:\n",
    "\n",
    "$\\sigma_{jk}=\\frac{1}{n−1}\\sum_{i = 1}^N = (x_{ij}−\\bar{x_{j}})(x_{ik}−\\bar{x_{k}})$.\n",
    "\n",
    "We can summarize the calculation of the covariance matrix via the following matrix equation:\n",
    "$\\sum_=\\frac{1}{n−1}((X−\\bar{x})^T(X−\\bar{x}))$\n",
    "\n",
    "where $\\bar{x}$ is the mean vector $\\bar{x}=\\sum_{i = 1}^n x_{i}.$\n",
    "\n",
    "The mean vector is a d-dimensional vector where each value in this vector represents the sample mean of a feature column in the dataset.\n",
    "\n",
    "\n",
    "#### EigenValues and EigenVectors\n",
    "\n",
    "In essence, an eigenvector v of a linear transformation T is a non-zero vector that, when T is applied to it, does not change direction. Applying T to the eigenvector only scales the eigenvector by the scalar value λ, called an eigenvalue. This condition can be written as the equation ${\\displaystyle T(\\mathbf {v} )=\\lambda \\mathbf {v} ,} $ then the eigenvalue equation above for a linear transformation can be rewritten as the matrix multiplication ${\\displaystyle Av=\\lambda v} $\n",
    "\n",
    "then v is an eigenvector of the linear transformation A and the scale factor λ is the eigenvalue corresponding to that eigenvector. Equation is the eigenvalue equation for the matrix A. Equation  can be stated equivalently as ${\\displaystyle (A-\\lambda I)v=0}$ where I is the n by n identity matrix.\n",
    "\n",
    "Equation has a non-zero solution $v$ if and only if the determinant of the matrix $(A − λI)$ is zero. Therefore, the eigenvalues of $A$ are values of $λ$ that satisfy the equation ${\\displaystyle |A-\\lambda I|=0}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sort eigenvalues\n",
    "The typical goal of a PCA is to reduce the dimensionality of the original feature space by projecting it onto a smaller subspace, where the eigenvectors will form the axes.\n",
    "\n",
    "In order to decide which eigenvector(s) can dropped without losing too much information for the construction of lower-dimensional subspace, we need to inspect the corresponding eigenvalues: The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped.\n",
    "In order to do so, the common approach is to rank the eigenvalues from highest to lowest in order choose the top $k$ eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Construct the projection matrix\n",
    "\n",
    "It's about time to get to the really interesting part: The construction of the projection matrix that will be used to transform the dataset onto the new feature subspace. Although, the name \"projection matrix\" has a nice ring to it, it is basically just a matrix of our concatenated top $k$ eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Transform the original dataset\n",
    "\n",
    "In this last step we will use the $d*k$-dimensional projection matrix $W$ to transform our samples onto the new subspace via the equation\n",
    "$Y=X×W$, where $Y$ is matrix of our transformed samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing The Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data from csv file\n",
    "data = pd.read_csv('Wine.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting Them to Features and targets\n",
    "X = data.iloc[:, 0:13].values\n",
    "y = data.iloc[:, 13].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Covariance Matrix\n",
    "cov_matrix = np.cov(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting the Eign Values And EigenVecorts\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming a pair of Eign Values / Pairs\n",
    "eig_pairs = [(np.abs(eig_vals[i]) , eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sorting All of Them\n",
    "eig_pairs.sort(key = lambda x : x[0] , reverse= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for i in range(2):\n",
    "    final.append(eig_pairs[i][1].reshape(13,1))\n",
    "\n",
    "# Creating the Projection Matrix\n",
    "matrix_w = np.hstack((final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming The Data\n",
    "Y = X.dot(matrix_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting The Data\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "for i, j in enumerate(np.unique(y)):\n",
    "    plt.scatter(Y[y == j, 0], Y[y == j, 1],\n",
    "                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n",
    "plt.title('Customer_Segment Wine')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting The PCA From sklearn library\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "New = pca.fit_transform(X)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting The Data For Sklearn Result\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "for i, j in enumerate(np.unique(y)):\n",
    "    plt.scatter(New[y == j, 0], New[y == j, 1],\n",
    "                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n",
    "plt.title('Customer_Segment Wine')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Purly\n",
    "In This Section We Gonna Implement PCA Using Our Own Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matrix multiplication\n",
    "def matmul (A, B):\n",
    "    rows_A = len(A)\n",
    "    cols_A = len(A[0])\n",
    "    rows_B = len(B)\n",
    "    cols_B = len(B[0])\n",
    "\n",
    "    if cols_A != rows_B:\n",
    "        print('Cannot multiply the two matrices. Incorrect dimensions.')\n",
    "        return\n",
    "\n",
    "    C = [[0 for row in range(cols_B)] for col in range(rows_A)]\n",
    "    \n",
    "    for i in range(rows_A):\n",
    "        for j in range(cols_B):\n",
    "            for k in range(cols_A):\n",
    "                C[i][j] += A[i][k] * B[k][j]\n",
    "    return C\n",
    "\n",
    "# simple mean function\n",
    "def mean(a_list):\n",
    "    return sum(a_list)/len(a_list)\n",
    "\n",
    "# calculate the variance of a list\n",
    "def variance(a_list):\n",
    "    mean_value = mean(a_list)\n",
    "    squared = [(x-mean_value)**2 for x in a_list]\n",
    "    return sum(squared) / len(a_list)\n",
    "\n",
    "# calculate standard deviation of a vetor\n",
    "def std(a_list):\n",
    "    return (variance(a_list)**(1/2))\n",
    "\n",
    "# Standardization of a vector x = x - mean(x) / std(x)\n",
    "def stand_vactor(a):\n",
    "    result = []\n",
    "    meanA = mean(a)\n",
    "    stdA = std(a)\n",
    "    for i in a:\n",
    "        result.append((i - meanA)/ stdA)\n",
    "    return result \n",
    "\n",
    "# Standardization of a matrix x = x - mean(x) / std(x)\n",
    "def stand_matrix(a):\n",
    "    zipa= list(zip(*a))\n",
    "    result = []\n",
    "    for i in zipa:\n",
    "        result.append(stand_vactor(i))\n",
    "    return list(map(list,zip(*result)))\n",
    "\n",
    "# Calculate the covariance matrix for a dataset\n",
    "def covariance_matrix(matrix):\n",
    "    pass\n",
    "\n",
    "# calculate the eigen value and eigen vetcors of a matrix\n",
    "def eigns_things(matrix):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
